\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\graphicspath{ {./images/} }
\begin{document}

\newpage
\section{Understanding Regression}

From a young age, we learn to associate being truthful and kind to others as good traits and lying as a bad trait. We draw from experience that expressing good traits are usually rewarding in nature whereas bad traits do not. We understand that there exists a relationship between our actions and their consequences. Finding out this relationship between two events; an action and a consequence, is at the core of regression.\\

When given two sets of data, a regression analysis helps us understand the relationship between those two. For example, say you're a fresh college sophomore looking for a place to live. When browsing through places to live you notice a clear trend. The closer your address to your campus, the more expensive the rent.

\begin{figure}[h]
\includegraphics{table1.JPG}
\centering
\caption{Table showing distance from campus and cost of room}
\end{figure}

If we plot this into a graph, it looks like this:

\begin{figure}[h]
\includegraphics[width=8cm]{download.png}
\centering
\caption{Graph showing distance from campus vs cost of room}
\end{figure}

Here, we see a clear line that fits all the points perfectly. The slope of the line is -3 and the x-intercept is 9200.

\newpage
However data in real life looks something like this:

\begin{figure}[h]
\includegraphics[width=8cm]{download (1).png}
\centering
\caption{Distance from campus vs cost of room in real life}
\end{figure}

It is quite clear that finding a straight line that fits all the point is impossible to do. The best we can do is find a line that most satisfies the condition such that the distance from a point to the line is as minimum as possible. Formalizing this dependence in a form of a clear equation is regression.\\

\begin{figure}[h]
\includegraphics[width=8cm]{download (2).png}
\centering
\caption{A linear regression line through given points}
\end{figure}

A more formal definition states: {\textbf{a regression analysis is a set of statistical processes for estimating the relationship between a dependent variable and one or more independent variables.
}}\\

There exists many kind of regressions. In this reading, we will be looking at the two most common: \textbf{Linear} and \textbf{Logistic} regression

\section{Linear Regression}
Linear regression is the simplest form of regression. The equation for linear regression resembles one of the first equations we learn: the equation of a line in slope-intercept form:

\[ y = m.x + c \]

(This is obvious because, in regression, we are trying to fit a line that satisfies a set of points as closely as possible)\\ 
In linear regression analysis, we write the equation a little differently:

\[ y = \theta_0 + \theta_1.x \]

Here, the values for $\theta_0$ and $\theta_1$ are called biases. Depending on how many independent variables, we add additional theta components to our existing equation. For example, for three independent variables that influence a dependent variable, the equation looks like:

\[ y = \theta_0 + \theta_1.x_1 + \theta_2.x_2 + \theta_3.x_3 \]

In matrix form, the equation looks like:

\[ y = \theta T.X \]
, where $\theta$ is the row matrix containing all the values of biases [ $\theta_0$, $\theta_1$, $\theta_2$, $\theta_3$ ] and X is the row matrix containing all the values of input data [$x_0$, $x_1$, $x_2$, $x_3$] and $x_0$ = 1.

Linear regression is useful for predicting the continuous dependent variable with the help of independent variable(s). The value from a linear regression can exist from any range.

\newpage
\section{Logistic Regression}
Logistic regression is another popular form of regression. It is a non-linear function useful in classification problems and outputs a probability of occurrence rather than a concrete value. The equation for logistic regression is as follows:

\[ log \left[ \dfrac{y}{1 - y} \right]  = \theta_0 + \theta_1.x_1 + \theta_2.x_2 + \theta_3.x_3 + ... \]

Similar to linear regression, the values for $\theta_0$ and $\theta_1$ are called biases. Depending on how many independent variables, we add additional theta components to our existing equation.

Logistic regression is useful for predicting where the probabilities between two classes is required. To solve a logistic regression problem, we require additional concept such as gradient descent. The value from a logistic regression can exist only exist for a range between (0, 1).

\end{document}
